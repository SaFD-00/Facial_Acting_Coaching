{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aaf06b",
   "metadata": {
    "id": "74aaf06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from vision_model_final_sol.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "# 원하는 모델로 바꾸어 사용하기\n",
    "from vision_model_final_sol import Mini_Xception\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c3f295",
   "metadata": {
    "id": "21c3f295"
   },
   "outputs": [],
   "source": [
    "# (주의) OpenCV는 Colab에서 제대로 실행되지 않음.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6dbcc1",
   "metadata": {
    "id": "6b6dbcc1"
   },
   "outputs": [],
   "source": [
    "# 사용할 모델 선언하기\n",
    "mini_xception = Mini_Xception().to(device)\n",
    "\n",
    "# train이 아닌, evaluation 과정\n",
    "mini_xception.eval()\n",
    "\n",
    "# 기존에 학습한 모델 불러오기. XX에 epoch 번호 작성.\n",
    "checkpoint = torch.load('checkpoint/model_weights/weights_epoch_2.pth.tar', map_location=device)\n",
    "mini_xception.load_state_dict(checkpoint['mini_xception'])\n",
    "\n",
    "# Face detection을 위한 CascadeClassifier 모델 불러오기\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "face_detector = cv2.CascadeClassifier(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456ac218",
   "metadata": {
    "id": "456ac218"
   },
   "outputs": [],
   "source": [
    "def sort_vec(vec):\n",
    "    \"\"\"\n",
    "    7차원 감정 벡터의 순서를 일치시키기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    vec : numpy.ndarray, shape-(7,)\n",
    "        vision 모델의 출력값인 7차원 벡터.\n",
    "        fer2013 dataset의 emotion label의 순서대로 구한 감정 벡터. \n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    numpy.ndarray, shape-(7,)\n",
    "        NLP dataset의 emotion index의 순서대로 재정렬한 감정 벡터\n",
    "    \n",
    "    참고사항\n",
    "    -------\n",
    "    (fer2013 dataset)\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust', \n",
    "        2: 'Fear', \n",
    "        3: 'Happy', \n",
    "        4: 'Sad', \n",
    "        5: 'Surprise', \n",
    "        6: 'Neutral'\n",
    "        \n",
    "    (NLP dataset)\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'  \n",
    "    \"\"\"\n",
    "    \n",
    "    vec2 = [vec[2],vec[5],vec[0],vec[4],vec[6],vec[3],vec[1]]\n",
    "    \n",
    "    return np.array(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac5936b",
   "metadata": {
    "id": "8ac5936b"
   },
   "outputs": [],
   "source": [
    "def get_label_emotion(label):\n",
    "    \"\"\"\n",
    "    label 값에 대응되는 감정의 이름 문자열을 구하기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    label : int\n",
    "        emotion label 번호\n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    String\n",
    "        label 번호에 대응되는 감정의 이름 문자열\n",
    "    \n",
    "    참고사항\n",
    "    -------\n",
    "    (NLP dataset)\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'      \n",
    "    \"\"\"\n",
    "    \n",
    "    label_emotion_map = {\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'        \n",
    "    }\n",
    "    \n",
    "    return label_emotion_map[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b87a6c",
   "metadata": {
    "id": "88b87a6c"
   },
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "    \"\"\"\n",
    "    두 벡터 A, B의 코사인 유사도를 구하기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    A : numpy.ndarray, shape-(N,)\n",
    "    B : numpy.ndarray, shape-(N,)\n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    numpy.float64\n",
    "        두 벡터 A, B의 코사인 유사도 값      \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.dot(A, B)/(np.linalg.norm(A)*np.linalg.norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae7a134c",
   "metadata": {
    "id": "ae7a134c"
   },
   "outputs": [],
   "source": [
    "def predict_video(nlp_vec, sentence):\n",
    "    \"\"\"\n",
    "    웹캠을 통해 받아온 실시간 영상 속에서\n",
    "    1) CascadeClassifier (혹은 다른 모델)을 통해 얼굴을 탐지하고\n",
    "    2) Mini_Xception (혹은 다른 모델)을 통해 얼굴 표정으로부터 7차원 감정 벡터를 추출하여\n",
    "    3) sort_vec 함수를 통해 2)에서 구한 감정 벡터의 순서를 재정렬한 후\n",
    "    4) cos_sim 함수를 이용하여 입력받은 문장에서 추출한 7차원 감정 벡터와의 코사인 유사도를 계산.\n",
    "    5) OpenCV 라이브러리를 사용하여, 문장과 표정에서 추출한 각 감정, 그리고 표정 연기에 대한 점수(코사인 유사도)를 window 상에 시각화.\n",
    "    \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    nlp_vec : 입력한 문장에서 추출한 7차원 감정 벡터\n",
    "    sentence : 사용자가 표정 연기 연습의 목적으로 입력한 문장\n",
    "    \"\"\"\n",
    "    \n",
    "    # OpenCV 실시간 웹캠 영상 불러오기\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # 프레임의 사이즈 계산 (height, width 구하기)\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    size = (width, height)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # 문장의 최대 확률 감정\n",
    "            emotion_max = np.argmax(nlp_vec)\n",
    "            nlp_percentage = np.round(nlp_vec[emotion_max], 2)\n",
    "            nlp_emotion_label = get_label_emotion(emotion_max)\n",
    "            \n",
    "            # 한글 문장 출력을 위한 하단 색 띠\n",
    "            frame_pil = frame\n",
    "            frame_pil[height-70 : height, 0 : width] = (50, 100, 50)\n",
    "            \n",
    "            # 한글 문장 출력을 위한 PIL 라이브러리 사용\n",
    "            frame_pil = Image.fromarray(cv2.cvtColor(frame_pil, cv2.COLOR_BGR2RGB))\n",
    "            draw = ImageDraw.Draw(frame_pil)\n",
    "            \n",
    "            msg1 = \"emotion : \" + nlp_emotion_label + \" (\" + str(nlp_percentage) + \")\"\n",
    "            msg2 = \"\\\"\" + sentence + \"\\\"\"\n",
    "            font = ImageFont.truetype(\"Hancom Gothic Regular.ttf\", 20)\n",
    "            w1, h1 = font.getsize(msg1)\n",
    "            w2, h2 = font.getsize(msg2)\n",
    "            draw.text(((width-w1)/2, height-h1-40), msg1, font = font, fill = (255, 255, 255))\n",
    "            draw.text(((width-w2)/2, height-h2-10), msg2, font = font, fill = (255, 255, 255))\n",
    "            \n",
    "            # 한글 문장이 출력되어 있는 프레임\n",
    "            frame_GUI = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # faces : 얼굴 탐지 결과 얻어진, face(x,y,w,h)로 이루어진 sequential data \n",
    "            faces = face_detector.detectMultiScale(frame)\n",
    "            \n",
    "            \n",
    "            for face in faces:\n",
    "                (x,y,w,h) = face\n",
    "            \n",
    "                # 웹캠에서 인식한 얼굴을 모델에 넣어주기 위한 전처리\n",
    "                '''\n",
    "                전처리 후, input_face에 저장\n",
    "                 1) face의 좌표에 따라 얼굴 부분 프레임만 잘라내기\n",
    "                 2) BGR2GRAY로 흑백 변환하기\n",
    "                 3) (48,48)로 resize\n",
    "                 4) 히스토그램 평활화 적용\n",
    "                 5) Tensor로 바꾸고 device에 저장\n",
    "                 6) (1,48,48)로 차원 증가\n",
    "                '''\n",
    "                input_face = frame[y:y+h, x:x+w]\n",
    "                input_face = cv2.cvtColor(input_face, cv2.COLOR_BGR2GRAY)\n",
    "                input_face = cv2.resize(input_face, (48,48))\n",
    "                input_face = cv2.equalizeHist(input_face)\n",
    "                input_face = transforms.ToTensor()(input_face).to(device)\n",
    "                input_face = torch.unsqueeze(input_face, 0)\n",
    "\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # 모델 출력값의 shape : [1, 7, 1, 1]\n",
    "                    emotion_vec = mini_xception(input_face).squeeze()\n",
    "                    \n",
    "                    # 7차원 감정 확률 벡터\n",
    "                    softmax = torch.nn.Softmax()\n",
    "                    vision_vec = softmax(emotion_vec)\n",
    "                    vision_vec = vision_vec.reshape(-1,1).cpu().detach().numpy()\n",
    "                    \n",
    "                    # 코사인 유사도 점수\n",
    "                    vision_vec = sort_vec(vision_vec)\n",
    "                    similarity = cos_sim(nlp_vec,vision_vec)\n",
    "                    \n",
    "                    # GUI 상에서 출력할 정보\n",
    "                    '''\n",
    "                     1) 한글 문장과 최대 확률 감정, 그 확률 (이미 PIL 라이브러리로 해결)\n",
    "                     2) 코사인 유사도 점수, score\n",
    "                     3) 표정의 최대 확률 감정과 그 확률\n",
    "                    '''\n",
    "                    score = np.round(similarity * 100, 2)\n",
    "                    \n",
    "                    emotion_max = np.argmax(vision_vec)\n",
    "                    vision_percentage = np.round(vision_vec[emotion_max], 2)\n",
    "                    vision_emotion_label = get_label_emotion(emotion_max)\n",
    "                    \n",
    "                    # 얼굴 표정 주변의 정보 출력을 위한 OpenCV 라이브러리 사용\n",
    "                    cv2.rectangle(frame_GUI, (x, y), (x + w, y + h), (255,0,0), 3)\n",
    "                    frame_GUI[y - 60 : y, x : x + w] = (50,50,50)\n",
    "                    frame_GUI[y + h - 60 : y, x : x + w] = (70,30,30)\n",
    "                    \n",
    "                    cv2.putText(frame_GUI, \"Score: \" + str(score), (x, y - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,200,200))\n",
    "                    cv2.putText(frame_GUI, vision_emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,200,200))\n",
    "                    cv2.putText(frame_GUI, str(vision_percentage), (x + w - 50, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200,200,0))            \n",
    "            \n",
    "            cv2.imshow(\"Video\", frame_GUI)\n",
    "            # 탈출 조건 : esc ( OxFF==27 )\n",
    "            if cv2.waitKey(1) & 0xff == 27:\n",
    "                break\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # 종료\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vision_main_final_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
